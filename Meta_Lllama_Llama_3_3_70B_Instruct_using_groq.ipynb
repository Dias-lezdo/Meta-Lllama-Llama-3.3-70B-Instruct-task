{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dias-lezdo/Meta-Lllama-Llama-3.3-70B-Instruct-task/blob/main/Meta_Lllama_Llama_3_3_70B_Instruct_using_groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dx9oDpLybc4H"
      },
      "outputs": [],
      "source": [
        "!pip install -q groq PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from groq import Groq\n",
        "import PyPDF2\n",
        "client = Groq(api_key=\"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\")"
      ],
      "metadata": {
        "id": "DTna4JfEbk73"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_pagewise_text(pdf_path, output_text_path):\n",
        "    \"\"\"Extract pagewise data from a PDF and save it to a text file.\"\"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            # Create PDF reader object\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            with open(output_text_path, \"w\") as output_file:\n",
        "                for page_num, page in enumerate(reader.pages, start=1):\n",
        "                    # Extract text from the current page\n",
        "                    text = page.extract_text().strip()\n",
        "                    if text:  # Skip empty pages\n",
        "                        output_file.write(f\"Page {page_num}:\\n\")\n",
        "                        output_file.write(text + \"\\n\\n\")\n",
        "            print(f\"Pagewise text saved to {output_text_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "\n",
        "# Path to your PDF file\n",
        "pdf_path = \"/content/Morgan_ Tujuanza - OCR_removed 10page.pdf\"\n",
        "\n",
        "# Path to save the extracted text\n",
        "output_text_path = \"Morgan.txt\"\n",
        "\n",
        "# Extract pagewise data and save to text file\n",
        "pdf_to_pagewise_text(pdf_path, output_text_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCKVKeGEb892",
        "outputId": "f4408f72-0177-42f0-d7c9-48854c519f6e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pagewise text saved to Morgan.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def chunk_text(text, chunk_size, overlap):\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk = \" \".join(tokens[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "        if i + chunk_size >= len(tokens):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "def process_chunk(chunk, client):\n",
        "    \"\"\"Process a single chunk using the RAG prompt logic.\"\"\"\n",
        "    rag_prompt = (\n",
        "        \"The following text is retrieved from a document, indexed by pages in chronological order:\\n\\n\"\n",
        "        f\"{chunk}\\n\\n\"\n",
        "        \"Document Content:\\n\\n\"\n",
        "        \"Your Task:\\n\"\n",
        "        \"1. Analyze the document content to perform the following tasks:\\n\"\n",
        "        \"   - Group the content by **evaluation date** ('Date of Service') and **Title**. Entries with the same 'Date of Service' and 'Title' must be consolidated into a single entry.\\n\"\n",
        "        \"   - Consolidate page numbers into a single range (e.g., 1-3) for grouped entries. Ensure that entries with different 'Date of Service' values or 'Title' are not combined, even if their content appears sequentially.\\n\"\n",
        "        \"   - Ignore any page numbers mentioned inside the 'Details' field and focus only on the 'Page Number' field for grouping.\\n\"\n",
        "        \"   - Include only the relevant content and exclude sections such as **Assessment**, **Plan**, **Review of Systems**, and **Physical Examination** from visit types.\\n\"\n",
        "        \"2. Determine the **Facility/Provider** based on the following rules, ensuring accuracy and consistency:\\n\"\n",
        "        \"   - If a **signed provider name** exists at the bottom of the page, treat it as the 'Treating Provider.'\\n\"\n",
        "        \"   - If both a signed provider and an attending provider are present on the page, prioritize the signed provider **only if they are explicitly indicated as the treating provider**. If there is ambiguity, include both providers, but list the signed provider first.\\n\"\n",
        "        \"   - Cross-check the signed provider at the bottom of the page with the provider name at the top of the page for consistency. If the names match, use the signed provider. If they do not match, treat the signed provider as the default unless there's a clear indication otherwise.\\n\"\n",
        "        \"   - If no signed provider exists at the bottom of the page, use the provider name mentioned at the top of the page, if available.\\n\"\n",
        "        \"   - If no provider name exists on the page (neither at the top nor signed at the bottom), mark the provider as 'Unknown provider.'\\n\"\n",
        "        \"   - Format the provider's name with their designation (e.g., 'Dr. B Bandhu, M.D.') and the complete center name (e.g., 'Georgia Spine and Orthopaedics').\\n\"\n",
        "        \"3. Collect the **Specialty** of the provider if explicitly mentioned in the document. If not mentioned, give *unknown*.\\n\"\n",
        "        \"4. Capture the following information for each grouped entry:\\n\"\n",
        "        \"   - **Date of Service**: The date of the visit, surgery, or progress note.\\n\"\n",
        "        \"   - **Facility/Provider**: The determined provider name and center based on the above logic.\\n\"\n",
        "        \"   - **Title**: Identify the type of report from the content (e.g., Consultation, Surgery, Progress Note).\\n\"\n",
        "        \"   - **Specialty**: The specialty of the provider, if available.\\n\"\n",
        "        \"   - **Page Number**: Consolidate the page numbers from the 'Page Number' field in the JSON into a range (e.g., 1-3), ensuring that entries with different 'Date of Service' or 'Title' are not combined.\\n\"\n",
        "        \"5. Validation Rules:\\n\"\n",
        "        \"   - Ensure that:\\n\"\n",
        "        \"       * All dates are formatted as MM/DD/YYYY.\\n\"\n",
        "        \"       * No textual or non-standard date formats are used.\\n\"\n",
        "        \"       * Do not hallucinate or infer content not explicitly mentioned in the source document.\\n\"\n",
        "        \"6. Chronological and Logical Organization:\\n\"\n",
        "        \"   - Reorganize shuffled pages strictly within the JSON-provided page range.\\n\"\n",
        "        \"   - Arrange reports chronologically based on the Date of Service (DOS).\\n\"\n",
        "        \"   - If dates are unavailable or illegible, maintain the listed order from the JSON key.\\n\"\n",
        "        \"7. Deduplication Rules:\\n\"\n",
        "        \"   - Identify and remove duplicate entries based on the following fields:\\n\"\n",
        "        \"       * 'Date of Service'\\n\"\n",
        "        \"       * 'Facility/Provider'\\n\"\n",
        "        \"       * 'Title'\\n\"\n",
        "        \"       * 'Specialty'\\n\"\n",
        "        \"   - If entries match across all fields, merge their page numbers into a single range (e.g., '1-3') and keep only one consolidated entry.\\n\"\n",
        "        \"8. Format the output into a JSON structure with the following schema:\\n\"\n",
        "        \"[\\n\"\n",
        "        \"    {\\n\"\n",
        "        \"        \\\"Date of Service\\\": \\\"MM-DD-YYYY\\\",\\n\"\n",
        "        \"        \\\"Facility/Provider\\\": \\\"Provider Name - Center Name\\\",\\n\"\n",
        "        \"        \\\"Title\\\": \\\"Type of Report\\\",\\n\"\n",
        "        \"        \\\"Specialty\\\": \\\"Specialty\\\",\\n\"\n",
        "        \"        \\\"Page Number\\\": \\\"Page Range\\\"\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"]\\n\"\n",
        "        \"Respond only with the JSON. Do not provide any additional text or reasoning.\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an intelligent assistant tasked with answering queries by strictly analyzing the provided context from a document.\"},\n",
        "            {\"role\": \"user\", \"content\": rag_prompt}\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0.6,\n",
        "        max_tokens=6000,\n",
        "        top_p=1,\n",
        "        stop=None,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    return json.loads(chat_completion.choices[0].message.content)\n",
        "\n",
        "def process_pagewise_data(pagewise_data, client, chunk_size=512, overlap=128):\n",
        "    \"\"\"Process the pagewise data in chunks.\"\"\"\n",
        "    chunks = chunk_text(pagewise_data, chunk_size, overlap)\n",
        "    final_results = []\n",
        "\n",
        "    print(f\"Total chunks: {len(chunks)}\")\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Processing chunk {i + 1}/{len(chunks)}...\")\n",
        "        torch.cuda.empty_cache()\n",
        "        chunk_result = process_chunk(chunk, client)\n",
        "        final_results.extend(chunk_result)\n",
        "\n",
        "    return final_results\n",
        "\n",
        "# Read text from the file\n",
        "with open(\"/content/Morgan.txt\", \"r\") as file:\n",
        "    pagewise_data = file.read()\n",
        "\n",
        "# Assuming `client` is defined and initialized\n",
        "final_json_results = process_pagewise_data(pagewise_data, client)\n",
        "\n",
        "# Save or print the results\n",
        "print(json.dumps(final_json_results, indent=4))\n"
      ],
      "metadata": {
        "id": "4gJ9ATWKbyEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**OUTPUT FORMAT**\n",
        "\n",
        "[\n",
        "\n",
        "{\n",
        "        \"Date of Service\": \"05/12/2022\",\n",
        "        \"Facility/Provider\": \"Erik Bendiks, MD - Georgia Spine and Orthopaedics\",\n",
        "        \"Title\": \"Progress Note\",\n",
        "        \"Specialty\": \"Orthopaedics\",\n",
        "        \"Page Number\": \"1-2\"\n",
        "    },\n",
        "    {\n",
        "        \"Date of Service\": \"05/11/2022\",\n",
        "        \"Facility/Provider\": \"Erik T Bendiks MD - Georgia Spine and Orthopaedics\",\n",
        "        \"Title\": \"Progress Note\",\n",
        "        \"Specialty\": \"unknown\",\n",
        "        \"Page Number\": \"9-10\"\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "EvNDjsPGBcFf"
      }
    }
  ]
}